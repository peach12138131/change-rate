import requests
import json
from datetime import datetime
import os
import random
from typing import List, Dict, Any, Optional,Generator
import time
from auto_article_config import keyword_list,claude_key,tavily_key,openai_key,generate_svg_prompt,rewrite_prompt,extract_svg_from_text, extract_final_report,news_schema,seo_metadata,seo_keywords,seo_link,seo_rewrite_prompt

def query_gpt_model(prompt: str, article: str, api_key: str, base_url: str = "https://api.anthropic.com/v1", 
                   model: str = "claude-sonnet-4-20250514", max_tokens: int = 10240, 
                   temperature: float = 0.0) -> Optional[str]:
  
    url = f"{base_url}/messages"
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": f"{prompt}\n \n{article}"}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        response_json = response.json()
        time.sleep(10)
        if "content" in response_json and len(response_json["content"]) > 0:
           
            text_content = response_json["content"][0]["text"]
            return text_content
        else:
            print("APIè¿”å›å†…å®¹æ ¼å¼å¼‚å¸¸")
            return None
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        return None



def query_openai_model(prompt: str, article: str, api_key: str, base_url: str = "https://api.openai.com/v1", 
                       model: str = "gpt-4.1-2025-04-14", max_tokens: int = 10240, 
                       temperature: float = 0.8,json_schema: dict = None) -> Optional[str]:
    
    url = f"{base_url}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": f"{prompt}\n \n{article}"}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    if json_schema:
        payload["response_format"] = {
            "type": "json_schema",
            "json_schema": json_schema
        }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        response_json = response.json()
        time.sleep(10)
        if "choices" in response_json and len(response_json["choices"]) > 0:
            text_content = response_json["choices"][0]["message"]["content"]
            return text_content
        else:
            print("APIè¿”å›å†…å®¹æ ¼å¼å¼‚å¸¸")
            return None
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        return None



def send_research_request(query="defualt"):
    """å‘æœ¬åœ°æ·±åº¦ç ”ç©¶æœåŠ¡å‘é€è¯·æ±‚"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_dir = "./temp_search_result"
    os.makedirs(output_dir, exist_ok=True)  # exist_ok=True è¡¨ç¤ºç›®å½•å­˜åœ¨æ—¶ä¸æŠ¥é”™
    filename = f"{output_dir}/deesearch_report_{timestamp}.txt"
    url = "http://47.237.119.79:8861/api/sse"
  

    # config = {
    #     "query": query,
    #     "provider": "anthropic",
    #     "thinkingModel": "claude-sonnet-4-20250514",
    #     "taskModel": "claude-sonnet-4-20250514",
    #     "searchProvider":"tavily",
    #     "language": "en-US",
    #     "maxResult": 5,
    #     "enableCitationImage": True,
    #     "enableReferences": True
    # }
    
    config = {
        "query": query,
        "provider": "openai",
        "thinkingModel": "gpt-4.1-2025-04-14",
        "taskModel": "gpt-4.1-2025-04-14",
        "searchProvider": "tavily",
        "language": "en-US",
        "maxResult": 5,
        "enableCitationImage": False,
        "enableReferences": True
    }
    
    try:
        # å‘é€è¯·æ±‚ï¼Œå¯ç”¨æµå¼å¤„ç†
        response = requests.post(url, json=config, stream=True,timeout=(60,6000)) #å®¹å¿åº¦é«˜,è¶…æ—¶æ—¶é—´è¦å¾ˆé•¿ä¿è¯æœç´¢,
        response.raise_for_status()
        
        print("è¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
        print("çŠ¶æ€ç :", response.status_code)
        print("=" * 50)
        final_report = ""
        current_event = None
        last_content = ""
        # å¤„ç†æµå¼å“åº”
        for line in response.iter_lines(decode_unicode=True):
            if line.strip():
               
                # print(line)
                if line.startswith('event: '):
                    # è§£æäº‹ä»¶ç±»å‹
                    current_event = line[7:].strip()
                    # print(f"\n[äº‹ä»¶]: {current_event}")

                elif line.startswith('data: '):
                    # æå–dataéƒ¨åˆ†
                    data = line[6:].strip()
                    
                    # æ¸…ç†æœ«å°¾çš„å¤šä½™å­—ç¬¦ )}
                    if data.endswith(')}'):
                        data = data[:-2]

                    if data:
                        try:
                            # è§£æJSONæ•°æ®
                            json_data = json.loads(data)
                            # print(f"\n[æ•°æ®]: {json_data}")
                            if current_event == "message" and json_data.get("type") == "text":
                                text_content = json_data.get("text", "")
                                if text_content== last_content:
                                    continue
                                else:
                                    final_report += text_content
                                last_content = text_content
                                # print(f"\n[æ–‡æœ¬å†…å®¹]: {text_content}")

                                yield text_content


                                


                        except json.JSONDecodeError as e:
                            print(f"\n[JSONè§£æé”™è¯¯]: {e}")
                            print(f"[åŸå§‹æ•°æ®]: {repr(data)}")



        print("\n" + "=" * 50)
        print("æ•°æ®æ¥æ”¶å®Œæˆ")
        print(f"ç´¯ç§¯æ–‡æœ¬é•¿åº¦: {len(final_report)} å­—ç¬¦")

        if final_report:
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(final_report)
            print("æŠ¥å‘Šå·²ä¿å­˜åˆ° weather_report.txt")

        
    except requests.exceptions.ConnectionError:
        print("[è¡¨æƒ…] è¿æ¥å¤±è´¥: è¯·ç¡®ä¿æœ¬åœ°æœåŠ¡å·²å¯åŠ¨ (pnpm dev)")
    except requests.exceptions.HTTPError as e:
        print(f"[è¡¨æƒ…] HTTPé”™è¯¯: {e}")
        if 'response' in locals():
            print("å“åº”å†…å®¹:", response.text)
    except requests.exceptions.RequestException as e:
        print(f"[è¡¨æƒ…] è¯·æ±‚é”™è¯¯: {e}")
    except Exception as e:
        print(f"[è¡¨æƒ…] å…¶ä»–é”™è¯¯: {e}")






def search_news(tavily_api_key: str=tavily_key, query: str="", max_results: int = 80, days: int = 3, 
                search_depth: str = "basic", include_answer: bool = True) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨Tavily APIæœç´¢æ–°é—»
    
    Args:
        tavily_api_key: Tavily APIå¯†é’¥
        query: æœç´¢æŸ¥è¯¢è¯
        max_results: æœ€å¤§è¿”å›ç»“æœæ•° (é»˜è®¤10)
        days: æœç´¢æ—¶é—´èŒƒå›´(å¤©) (é»˜è®¤7å¤©)
        search_depth: æœç´¢æ·±åº¦ "basic" æˆ– "advanced" (é»˜è®¤basic)
        include_answer: æ˜¯å¦åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦ (é»˜è®¤True)
    
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„å­—å…¸åˆ—è¡¨
    """
    url = "https://api.tavily.com/search"
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {tavily_api_key}'
    }
    
    data = {
        "query": query,
        "topic": "news",  # ä¸“é—¨æœç´¢æ–°é—»
        "search_depth": search_depth,  # æœç´¢æ·±åº¦
        "chunks_per_source": 3,  # æ¯ä¸ªæ¥æºçš„å†…å®¹å—æ•°
        "max_results": max_results,  # æœ€å¤§ç»“æœæ•°
        "time_range": None,  # æ—¶é—´èŒƒå›´(nullè¡¨ç¤ºä½¿ç”¨dayså‚æ•°)
        "days": days,  # æœç´¢æœ€è¿‘Nå¤©çš„æ–°é—»
        "include_answer": include_answer,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        "include_raw_content": False,  # ä¸åŒ…å«åŸå§‹HTMLå†…å®¹
        "include_images": False,  # åŒ…å«å›¾ç‰‡
        "include_image_descriptions": False,  # ä¸åŒ…å«å›¾ç‰‡æè¿°
        "include_domains": [],  # åŒ…å«çš„åŸŸååˆ—è¡¨(ç©ºè¡¨ç¤ºä¸é™åˆ¶)
        "exclude_domains": [],  # æ’é™¤çš„åŸŸååˆ—è¡¨
        "country": None  # å›½å®¶é™åˆ¶(nullè¡¨ç¤ºå…¨çƒ)
    }
    
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢æ–°é—»: {query}")
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        
        result = response.json()
        
        # æ£€æŸ¥APIå“åº”çŠ¶æ€
        if 'results' not in result:
            print(f"âš ï¸ APIå“åº”å¼‚å¸¸: {result}")
            return []
        
        results = result.get("results", [])
        answer = result.get("answer", "")  # AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        
        print(f"âœ… æ‰¾åˆ° {len(results)} æ¡æ–°é—»ç»“æœ\n")
        print(results)
        
        # å¦‚æœåŒ…å«AIç­”æ¡ˆæ‘˜è¦ï¼Œæ‰“å°å‡ºæ¥
        if include_answer and answer:
            print(f"ğŸ“ AIæ‘˜è¦: {answer[:]}...")
        
        return answer,results
        
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}")
        if hasattr(e.response, 'text'):
            print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return []
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return []







def auto_write_article(news_list):
    """è‡ªåŠ¨å†™æ–‡ç« çš„ä¸»å‡½æ•°"""
    
    
    try:
        news_list=news_list 
       
        # å¯¹æ¯æ¡è¿›è¡Œæ·±åº¦æœç´ ,å¯¹æ¯æ¡è¿›è¡Œæ·±åº¦æœç´¢å¹¶ç”Ÿæˆå•ç‹¬æ–‡ç« 
        for i, news in enumerate(news_list, 1): 
            category = news.get("category", "")
            keyword = news.get("keyword_en", "")
            description= news.get("description", "")
            print(f"\n[è¡¨æƒ…] æ­£åœ¨ç ”ç©¶ç¬¬ {i} æ¡æ–°é—»:")
            print(f"æ ‡é¢˜: {category}")
            print(f"å†…å®¹: {description}")

            #è·å–å®æ—¶æ–°é—»ä¸»é¢˜
            topic,news_results=search_news(query=f"Search for diversity news releated to private jet (or business charter) about:{category},keyword:{keyword}Please find relevant news articles, expert opinions, and background information.")

            print(f"ä¸»é¢˜: {news_results}")

            extract_query =f"Please extract the news that most relevant to private charter topic about:{category},{description} from the following news results.Note:1.the content should be related 2.Try choose diverse sources"

            extract_news = query_openai_model(extract_query,news_results,openai_key,json_schema=news_schema)
            print(f"æå–çš„æ–°é—»: {extract_news}")

            # # æ„å»ºç ”ç©¶æŸ¥è¯¢
            # query = f"Please research the related information and related news releated to private jet (or business charter) about this news topic: {news_results}"
            
            # # è¿›è¡Œæ·±åº¦ç ”ç©¶
            # print("[è¡¨æƒ…] å¼€å§‹æ·±åº¦ç ”ç©¶...")
            # research_content = ""
            # for content_chunk in send_research_request(query=query):
            #     research_content += content_chunk
            
            # 


            if extract_news:
                # æå–æœ€ç»ˆæŠ¥å‘Š
                # final_report = extract_final_report(research_content)
                final_report = extract_news
                
                # æ„å»ºåŸå§‹logå›æº¯
                article_content = f"# {category}\n\n## description\n{description}\n\n## the research material and collected news \n{final_report}"
                
                
                # è°ƒç”¨APIæ”¹å†™æ–‡ç« 
                print(f" æ­£åœ¨ç”Ÿæˆç¬¬ {i} æ¡æ–°é—»çš„æ–‡ç« ...")
                if random.choice([True, False]):
                    rewritten_article = query_gpt_model(rewrite_prompt, article_content, claude_key, temperature=1.0)
                    model_used = "claude"
                else:
                    rewritten_article = query_openai_model(rewrite_prompt, article_content, openai_key, temperature=1.0)
                    model_used = "OpenAI"

                print(f"[è¡¨æƒ…] ä½¿ç”¨äº† {model_used} æ¨¡å‹")
                
                
                #seoæ”¹å†™æµç¨‹1ï¼Œæå–å…³é”®å­—
                keywords_prompt=seo_keywords.format(rewritten_article)
                keywords=query_gpt_model(keywords_prompt, "", claude_key, temperature=1.0)


                #2ç”Ÿæˆmetadata
                metadata_prompt=seo_metadata.format(rewritten_article,keywords)
                metadata=query_gpt_model(metadata_prompt, "", claude_key, temperature=1.0)

                #3é‡å†™
                seo_rewrite=seo_rewrite_prompt.format(final_report,metadata)
                seo_article=query_gpt_model(seo_rewrite, "", claude_key, temperature=1.0)
                
                #æ¤å…¥é“¾æ¥
                seo_link_prompt=seo_link.format(seo_article)
                final_seo_article=query_gpt_model(seo_link_prompt, "", claude_key, temperature=1.0)
                final_seo_article=f"keywords\n{keywords}\n\n{final_seo_article}"

                log_content=f"collected news \n{final_report} \n \nkeywords\n{keywords}\n\n"





                if rewritten_article:
                    date_str = datetime.now().strftime("%Y%m%d")
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    
                    # æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
                    safe_title = "".join(c for c in category if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
                    
                    # åˆ›å»ºå¸¦æ—¥æœŸçš„è¾“å‡ºç›®å½•
                    output_dir = f"./output/{date_str}"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # ä¿å­˜æ”¹å†™åçš„æ–‡ç« 
                    filename = f"{output_dir}/article_{i}_{safe_title}_{timestamp}.txt"
                    with open(filename, 'w', encoding='utf-8') as f:
                        f.write(rewritten_article)

                    #ä¿å­˜seoæ–‡ç« 
                    seo_filename = f"{output_dir}/seo_article_{i}_{safe_title}_{timestamp}.txt"
                    with open(seo_filename, 'w', encoding='utf-8') as f:
                        f.write(final_seo_article)


                    # ä¿å­˜SVGæ–‡ä»¶ - å…³é”®ä»£ç å°±è¿™å‡ è¡Œ
                    svg_text = query_gpt_model(generate_svg_prompt, rewritten_article, claude_key)
                    svg_codes=extract_svg_from_text(svg_text)
                    for idx, svg_code in enumerate(svg_codes):
                        svg_filename = f"{output_dir}/chart_{i}_{safe_title}_{timestamp}_{idx+1}.svg"
                        with open(svg_filename, 'w', encoding='utf-8') as f:
                            f.write(svg_code)
                        print(f"[æˆåŠŸ] SVGå·²ä¿å­˜: {svg_filename}")
                    

                    # åŒæ—¶ä¿å­˜åŸå§‹ç ”ç©¶å†…å®¹
                    raw_filename = f"{output_dir}/raw_research_{i}_{safe_title}_{timestamp}.txt"
                    with open(raw_filename, 'w', encoding='utf-8') as f:
                        f.write(log_content)
                else:
                    print(f"[è¡¨æƒ…] ç¬¬ {i} æ¡æ–°é—»æ–‡ç« ç”Ÿæˆå¤±è´¥")
                    
                print(f"[è¡¨æƒ…] ç¬¬ {i} æ¡æ–°é—»å¤„ç†å®Œæˆ")
            else:
                print(f"[è¡¨æƒ…] ç¬¬ {i} æ¡æ–°é—»ç ”ç©¶å¤±è´¥")

        print("\n[è¡¨æƒ…] æ‰€æœ‰æ–°é—»å¤„ç†å®Œæˆï¼")
            
    except Exception as e:
        print(f"[è¡¨æƒ…] å¤„ç†å¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    import schedule
    import time
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    auto_write_article(news_list=keyword_list)

    # è®¾ç½®æ¯å¤©10ç‚¹æ‰§è¡Œ
    schedule.every().day.at("10:00").do(auto_write_article, news_list=keyword_list)

    # ä¿æŒè¿è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)
    
 
    
