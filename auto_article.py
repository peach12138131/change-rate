import requests
import json
from datetime import datetime
import os
import random
from typing import List, Dict, Any, Optional,Generator
from collections import defaultdict
import time
from auto_article_config import keyword_list,claude_key,tavily_key,openai_key,generate_svg_prompt,rewrite_prompt,extract_svg_from_text, extract_final_report,news_schema,seo_metadata,seo_keywords,seo_link,seo_rewrite_prompt

def query_gpt_model(prompt: str, article: str, api_key: str, base_url: str = "https://api.anthropic.com/v1", 
                   model: str = "claude-sonnet-4-20250514", max_tokens: int = 10240, 
                   temperature: float = 0.0) -> Optional[str]:
  
    url = f"{base_url}/messages"
    headers = {
        "Content-Type": "application/json",
        "x-api-key": api_key,
        "anthropic-version": "2023-06-01"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": f"{prompt}\n \n{article}"}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        response_json = response.json()
        time.sleep(30)
        if "content" in response_json and len(response_json["content"]) > 0:
           
            text_content = response_json["content"][0]["text"]
            return text_content
        else:
            print("APIè¿”å›å†…å®¹æ ¼å¼å¼‚å¸¸")
            return None
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        return None



def query_openai_model(prompt: str, article: str, api_key: str, base_url: str = "https://api.openai.com/v1", 
                       model: str = "gpt-4.1-2025-04-14", max_tokens: int = 10240, 
                       temperature: float = 0.8,json_schema: dict = None) -> Optional[str]:
    
    url = f"{base_url}/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": f"{prompt}\n \n{article}"}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens
    }
    
    if json_schema:
        payload["response_format"] = {
            "type": "json_schema",
            "json_schema": json_schema
        }
    
    try:
        response = requests.post(url, headers=headers, json=payload)
        response.raise_for_status()
        response_json = response.json()
        time.sleep(10)
        if "choices" in response_json and len(response_json["choices"]) > 0:
            text_content = response_json["choices"][0]["message"]["content"]
            return text_content
        else:
            print("APIè¿”å›å†…å®¹æ ¼å¼å¼‚å¸¸")
            return None
    except requests.exceptions.RequestException as e:
        print(f"APIè¯·æ±‚å¼‚å¸¸: {e}")
        if hasattr(e, 'response') and hasattr(e.response, 'text'):
            print(f"APIé”™è¯¯å“åº”: {e.response.text}")
        return None



def send_research_request(query="defualt"):
    """å‘æœ¬åœ°æ·±åº¦ç ”ç©¶æœåŠ¡å‘é€è¯·æ±‚"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    output_dir = "./temp_search_result"
    os.makedirs(output_dir, exist_ok=True)  # exist_ok=True è¡¨ç¤ºç›®å½•å­˜åœ¨æ—¶ä¸æŠ¥é”™
    filename = f"{output_dir}/deesearch_report_{timestamp}.txt"
    url = "http://47.237.119.79:8861/api/sse"
  

    # config = {
    #     "query": query,
    #     "provider": "anthropic",
    #     "thinkingModel": "claude-sonnet-4-20250514",
    #     "taskModel": "claude-sonnet-4-20250514",
    #     "searchProvider":"tavily",
    #     "language": "en-US",
    #     "maxResult": 5,
    #     "enableCitationImage": True,
    #     "enableReferences": True
    # }
    
    config = {
        "query": query,
        "provider": "openai",
        "thinkingModel": "gpt-4.1-2025-04-14",
        "taskModel": "gpt-4.1-2025-04-14",
        "searchProvider": "tavily",
        "language": "en-US",
        "maxResult": 5,
        "enableCitationImage": False,
        "enableReferences": True
    }
    
    try:
        # å‘é€è¯·æ±‚ï¼Œå¯ç”¨æµå¼å¤„ç†
        response = requests.post(url, json=config, stream=True,timeout=(60,6000)) #å®¹å¿åº¦é«˜,è¶…æ—¶æ—¶é—´è¦å¾ˆé•¿ä¿è¯æœç´¢,
        response.raise_for_status()
        
        print("è¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
        print("çŠ¶æ€ç :", response.status_code)
        print("=" * 50)
        final_report = ""
        current_event = None
        last_content = ""
        # å¤„ç†æµå¼å“åº”
        for line in response.iter_lines(decode_unicode=True):
            if line.strip():
               
                # print(line)
                if line.startswith('event: '):
                    # è§£æäº‹ä»¶ç±»å‹
                    current_event = line[7:].strip()
                    # print(f"\n[äº‹ä»¶]: {current_event}")

                elif line.startswith('data: '):
                    # æå–dataéƒ¨åˆ†
                    data = line[6:].strip()
                    
                    # æ¸…ç†æœ«å°¾çš„å¤šä½™å­—ç¬¦ )}
                    if data.endswith(')}'):
                        data = data[:-2]

                    if data:
                        try:
                            # è§£æJSONæ•°æ®
                            json_data = json.loads(data)
                            # print(f"\n[æ•°æ®]: {json_data}")
                            if current_event == "message" and json_data.get("type") == "text":
                                text_content = json_data.get("text", "")
                                if text_content== last_content:
                                    continue
                                else:
                                    final_report += text_content
                                last_content = text_content
                                # print(f"\n[æ–‡æœ¬å†…å®¹]: {text_content}")

                                yield text_content


                                


                        except json.JSONDecodeError as e:
                            print(f"\n[JSONè§£æé”™è¯¯]: {e}")
                            print(f"[åŸå§‹æ•°æ®]: {repr(data)}")



        print("\n" + "=" * 50)
        print("æ•°æ®æ¥æ”¶å®Œæˆ")
        print(f"ç´¯ç§¯æ–‡æœ¬é•¿åº¦: {len(final_report)} å­—ç¬¦")

        if final_report:
            
            with open(filename, "w", encoding="utf-8") as f:
                f.write(final_report)
            print("æŠ¥å‘Šå·²ä¿å­˜åˆ° weather_report.txt")

        
    except requests.exceptions.ConnectionError:
        print("[è¡¨æƒ…] è¿æ¥å¤±è´¥: è¯·ç¡®ä¿æœ¬åœ°æœåŠ¡å·²å¯åŠ¨ (pnpm dev)")
    except requests.exceptions.HTTPError as e:
        print(f"[è¡¨æƒ…] HTTPé”™è¯¯: {e}")
        if 'response' in locals():
            print("å“åº”å†…å®¹:", response.text)
    except requests.exceptions.RequestException as e:
        print(f"[è¡¨æƒ…] è¯·æ±‚é”™è¯¯: {e}")
    except Exception as e:
        print(f"[è¡¨æƒ…] å…¶ä»–é”™è¯¯: {e}")






def search_news(tavily_api_key: str=tavily_key, query: str="", max_results: int = 80, days: int = 3, 
                search_depth: str = "basic", include_answer: bool = True) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨Tavily APIæœç´¢æ–°é—»
    
    Args:
        tavily_api_key: Tavily APIå¯†é’¥
        query: æœç´¢æŸ¥è¯¢è¯
        max_results: æœ€å¤§è¿”å›ç»“æœæ•° (é»˜è®¤10)
        days: æœç´¢æ—¶é—´èŒƒå›´(å¤©) (é»˜è®¤7å¤©)
        search_depth: æœç´¢æ·±åº¦ "basic" æˆ– "advanced" (é»˜è®¤basic)
        include_answer: æ˜¯å¦åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦ (é»˜è®¤True)
    
    Returns:
        åŒ…å«æœç´¢ç»“æœçš„å­—å…¸åˆ—è¡¨
    """
    url = "https://api.tavily.com/search"
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {tavily_api_key}'
    }
    
    data = {
        "query": query,
        "topic": "news",  # ä¸“é—¨æœç´¢æ–°é—»
        "search_depth": search_depth,  # æœç´¢æ·±åº¦
        "chunks_per_source": 3,  # æ¯ä¸ªæ¥æºçš„å†…å®¹å—æ•°
        "max_results": max_results,  # æœ€å¤§ç»“æœæ•°
        "time_range": None,  # æ—¶é—´èŒƒå›´(nullè¡¨ç¤ºä½¿ç”¨dayså‚æ•°)
        "days": days,  # æœç´¢æœ€è¿‘Nå¤©çš„æ–°é—»
        "include_answer": include_answer,  # åŒ…å«AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        "include_raw_content": False,  # ä¸åŒ…å«åŸå§‹HTMLå†…å®¹
        "include_images": False,  # åŒ…å«å›¾ç‰‡
        "include_image_descriptions": False,  # ä¸åŒ…å«å›¾ç‰‡æè¿°
        "include_domains": [],  # åŒ…å«çš„åŸŸååˆ—è¡¨(ç©ºè¡¨ç¤ºä¸é™åˆ¶)
        "exclude_domains": [],  # æ’é™¤çš„åŸŸååˆ—è¡¨
        "country": None  # å›½å®¶é™åˆ¶(nullè¡¨ç¤ºå…¨çƒ)
    }
    
    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢æ–°é—»: {query}")
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯
        
        result = response.json()
        
        # æ£€æŸ¥APIå“åº”çŠ¶æ€
        if 'results' not in result:
            print(f"âš ï¸ APIå“åº”å¼‚å¸¸: {result}")
            return []
        
        results = result.get("results", [])
        answer = result.get("answer", "")  # AIç”Ÿæˆçš„ç­”æ¡ˆæ‘˜è¦
        
        print(f"âœ… æ‰¾åˆ° {len(results)} æ¡æ–°é—»ç»“æœ\n")
        
        
        # å¦‚æœåŒ…å«AIç­”æ¡ˆæ‘˜è¦ï¼Œæ‰“å°å‡ºæ¥
        if include_answer and answer:
            print(f"ğŸ“ AIæ‘˜è¦: {answer[:]}...")
        
        return answer, results
        
    except requests.exceptions.Timeout:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTPé”™è¯¯: {e}")
        if hasattr(e.response, 'text'):
            print(f"é”™è¯¯è¯¦æƒ…: {e.response.text}")
        return []
    except requests.exceptions.RequestException as e:
        print(f"âŒ è¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        return []







def auto_write_article(news_list):
    """è‡ªåŠ¨å†™æ–‡ç« çš„ä¸»å‡½æ•°"""
    
    
    try:
        news_pool=[]
        news_list=news_list 
        date_str = datetime.now().strftime("%Y%m%d")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # åˆ›å»ºå¸¦æ—¥æœŸçš„è¾“å‡ºç›®å½•
        output_dir = f"./output/{date_str}"
        os.makedirs(output_dir, exist_ok=True)
       
        # å¯¹å…³é”®è¯è¿›è¡Œæœç´¢ï¼Œç”Ÿæˆæ–°é—»æ± 
        for i, news in enumerate(news_list): 
            category = news.get("category", "")
            keyword = news.get("keyword_en", "")
            description= news.get("description", "")
            print(f"\n[è¡¨æƒ…] æ­£åœ¨ç ”ç©¶ç¬¬ {i} æ–°é—»:")
            print(f"æ ‡é¢˜: {category}")
            print(f"å†…å®¹: {description}")

            #è·å–å®æ—¶æ–°é—»ä¸»é¢˜
            topic,news_results=search_news(query=f"Search for diversity news releated to private jet (or business charter) about:{category},keyword:{keyword}Please find relevant news articles, expert opinions, and background information.")

           
            print("ç­›é€‰æ–°é—»ä¸­")
            extract_query =f"""Current time {date_str}. 
                                Please extract the news that most relevant to private charter topic about from the following news results.
                                Important Note:
                                1.the news content should be related 
                                2.Try choose diverse sources
                                3.Filter out the news too old 
                                4.Filter out news from which it's difficult to obtain useful information,such as those that are too short, too old, or irrelevant.
                                """
            extract_news = query_openai_model(extract_query,news_results,openai_key,json_schema=news_schema)

            print(extract_news)
            extract_news=json.loads(extract_news)
            print(extract_news["news_list"])
            

            news_pool.append(extract_news["news_list"])

           

            

         #å»å¯¹news_poolè¿›è¡Œå»é‡
        all_news = []
        for news_list in news_pool:
            all_news.extend(news_list)

        # ç”¨å­—å…¸å»é‡ï¼ŒURLä½œä¸ºkey
        unique_news_dict = {}
        for news in all_news:
            url = news.get('url')
            if url:
                unique_news_dict[url] = news  # å¦‚æœURLé‡å¤ï¼Œä¼šè¢«è¦†ç›–

        news_pool = list(unique_news_dict.values())

        #ä¿å­˜news_poolä¸ºjson
        news_file_name=os.path.join(output_dir, "jetbay_news_pool.json")
        with open(news_file_name, "w", encoding="utf-8") as f:
            json.dump(news_pool, f, ensure_ascii=False, indent=4)


       


        
            
           
        if news_pool:
            # æŒ‰ç±»åˆ«åˆ†ç»„æ–°é—»
            news_by_category = defaultdict(list)
            for news in news_pool:
                category = news.get('category', 'Unknown')
                news_by_category[category].append(news)
            
            print(f"å‘ç° {len(news_by_category)} ä¸ªç±»åˆ«çš„æ–°é—»")
            for category, news_list in news_by_category.items():
                print(f"ç±»åˆ« '{category}': {len(news_list)} æ¡æ–°é—»")
                start_idx = 0
                batch_count = 0

                while start_idx < len(news_list):
                    batch_size = min(random.randint(5, 12), len(news_list) - start_idx)
                    end_idx = start_idx + batch_size
                    print(f"\nå¤„ç†ç¬¬ {batch_count + 1} æ‰¹æ¬¡ï¼šç´¢å¼• {start_idx}-{end_idx-1} ({batch_size} æ¡æ–°é—»)")
                    batch_news = news_list[start_idx:end_idx]
                    final_report = ""
                    for new in enumerate(batch_news):
                        final_report += f"{new}\n\n"
                    
                    # æ„å»ºåŸå§‹logå›æº¯
                    article_content = f"## the research material and collected news \n{final_report}"
                    
                    
                    # è°ƒç”¨APIæ”¹å†™æ–‡ç« 
                    print(f" æ­£åœ¨ç”Ÿæˆç¬¬ {batch_count + 1} æ¡æ–°é—»çš„æ–‡ç« ...")
                    if random.choice([True, False]):
                        rewritten_article = query_gpt_model(rewrite_prompt, article_content, claude_key, temperature=1.0)
                        model_used = "claude"
                    else:
                        rewritten_article = query_openai_model(rewrite_prompt, article_content, openai_key, temperature=1.0)
                        model_used = "OpenAI"

                    print(f"[è¡¨æƒ…] ä½¿ç”¨äº† {model_used} æ¨¡å‹")
                    
                    
                    #seoæ”¹å†™æµç¨‹1ï¼Œæå–å…³é”®å­—
                    keywords_prompt=seo_keywords.format(rewritten_article)
                    keywords=query_gpt_model(keywords_prompt, "", claude_key, temperature=1.0)


                    #2ç”Ÿæˆmetadata
                    metadata_prompt=seo_metadata.format(rewritten_article,keywords)
                    metadata=query_gpt_model(metadata_prompt, "", claude_key, temperature=1.0)

                    #3é‡å†™
                    seo_rewrite=seo_rewrite_prompt.format(final_report,metadata)
                    seo_article=query_gpt_model(seo_rewrite, "", claude_key, temperature=1.0)
                    
                    #æ¤å…¥é“¾æ¥
                    seo_link_prompt=seo_link.format(seo_article)
                    final_seo_article=query_gpt_model(seo_link_prompt, "", claude_key, temperature=1.0)
                    final_seo_article=f"keywords\n{keywords}\n\n{final_seo_article}"

                    log_content=f"collected news \n{final_report} \n \nkeywords\n{keywords}\n\n"





                    if rewritten_article:
                        
                        
                        
                        # ä¿å­˜æ”¹å†™åçš„æ–‡ç« 
                        filename = f"{output_dir}/article_{batch_count + 1}_{timestamp}.txt"
                        with open(filename, 'w', encoding='utf-8') as f:
                            f.write(rewritten_article)

                        #ä¿å­˜seoæ–‡ç« 
                        jetbay_seo_dir=os.path.join(output_dir,"JETBAY")
                        os.makedirs(jetbay_seo_dir, exist_ok=True)

                        seo_filename = os.path.join(output_dir, 'JETBAY', f'seo_article_{batch_count + 1}_{timestamp}.txt')
                        with open(seo_filename, 'w', encoding='utf-8') as f:
                            f.write(final_seo_article)



                        # åŒæ—¶ä¿å­˜åŸå§‹ç ”ç©¶å†…å®¹
                        raw_filename = f"{output_dir}/raw_research_{batch_count + 1}_{timestamp}.txt"
                        with open(raw_filename, 'w', encoding='utf-8') as f:
                            f.write(log_content)
                    else:
                        print(f" ç¬¬ {batch_count + 1} æ¡æ–°é—»æ–‡ç« ç”Ÿæˆå¤±è´¥")
                        
                    print(f" ç¬¬ {batch_count + 1} æ¡æ–°é—»å¤„ç†å®Œæˆ")

                    start_idx = end_idx
                    batch_count += 1
            

        print("\n æ‰€æœ‰æ–°é—»å¤„ç†å®Œæˆï¼")
        return 0  
            
    except Exception as e:
        print(f" å¤„ç†å¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    import schedule
    import time 
    from datetime import datetime, timedelta

    max_retries = 3
    retry_interval_hours = 1
    retry_count = 0
    next_retry_time = None
    
    def execute_with_retry():
        """æ‰§è¡Œä»»åŠ¡å¹¶å¤„ç†é‡è¯•é€»è¾‘"""
        global retry_count, next_retry_time
        
        print(f"æ‰§è¡Œä»»åŠ¡... å°è¯•æ¬¡æ•°: {retry_count + 1}")
        res = auto_write_article(news_list=keyword_list)
        
        if res == 0:  # æˆåŠŸ
            print("ä»»åŠ¡æ‰§è¡ŒæˆåŠŸï¼")
            retry_count = 0
            next_retry_time = None
        else:  # å¤±è´¥
            retry_count += 1
            if retry_count < max_retries:
                next_retry_time = datetime.now() + timedelta(hours=retry_interval_hours)
                print(f"ä»»åŠ¡å¤±è´¥ï¼Œå°†åœ¨ {next_retry_time.strftime('%H:%M')} è¿›è¡Œç¬¬ {retry_count + 1} æ¬¡é‡è¯•...")
            else:
                print(f"ä»»åŠ¡å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° {max_retries}")
                retry_count = 0
                next_retry_time = None
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡
    print("ç«‹å³æ‰§è¡Œä¸€æ¬¡ä»»åŠ¡...")
    execute_with_retry()


    schedule.every().day.at("10:00").do(execute_with_retry)
    # ä¿æŒè¿è¡Œ
    while True:
        if next_retry_time and datetime.now() >= next_retry_time:
                execute_with_retry()
        schedule.run_pending()
        time.sleep(60)
    
 
    
